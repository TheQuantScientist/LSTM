{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a87b33ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "126/126 [==============================] - 5s 12ms/step - loss: 0.0436 - val_loss: 0.0670\n",
      "Epoch 2/50\n",
      "126/126 [==============================] - 1s 10ms/step - loss: 0.0027 - val_loss: 0.0296\n",
      "Epoch 3/50\n",
      "126/126 [==============================] - 2s 14ms/step - loss: 8.4630e-04 - val_loss: 0.0043\n",
      "Epoch 4/50\n",
      "126/126 [==============================] - 2s 15ms/step - loss: 3.3509e-04 - val_loss: 0.0024\n",
      "Epoch 5/50\n",
      "126/126 [==============================] - 1s 10ms/step - loss: 2.8953e-04 - val_loss: 0.0022\n",
      "Epoch 6/50\n",
      "126/126 [==============================] - 1s 10ms/step - loss: 2.5265e-04 - val_loss: 0.0016\n",
      "Epoch 7/50\n",
      "126/126 [==============================] - 1s 12ms/step - loss: 2.3392e-04 - val_loss: 9.8564e-04\n",
      "Epoch 8/50\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 2.2094e-04 - val_loss: 0.0012\n",
      "Epoch 9/50\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 2.3043e-04 - val_loss: 9.4482e-04\n",
      "Epoch 10/50\n",
      "126/126 [==============================] - 1s 10ms/step - loss: 2.2556e-04 - val_loss: 7.5673e-04\n",
      "Epoch 11/50\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 2.1219e-04 - val_loss: 8.7125e-04\n",
      "Epoch 12/50\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 2.1290e-04 - val_loss: 7.7450e-04\n",
      "Epoch 13/50\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 1.9860e-04 - val_loss: 6.6805e-04\n",
      "Epoch 14/50\n",
      "126/126 [==============================] - 1s 6ms/step - loss: 2.0976e-04 - val_loss: 7.0612e-04\n",
      "Epoch 15/50\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 2.1634e-04 - val_loss: 0.0010\n",
      "Epoch 16/50\n",
      "126/126 [==============================] - 1s 6ms/step - loss: 2.1489e-04 - val_loss: 6.5686e-04\n",
      "Epoch 17/50\n",
      "126/126 [==============================] - 1s 6ms/step - loss: 2.0367e-04 - val_loss: 6.5799e-04\n",
      "Epoch 18/50\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 2.0980e-04 - val_loss: 6.6293e-04\n",
      "Epoch 19/50\n",
      "126/126 [==============================] - 1s 7ms/step - loss: 1.9468e-04 - val_loss: 6.2777e-04\n",
      "Epoch 20/50\n",
      "126/126 [==============================] - 1s 6ms/step - loss: 1.7858e-04 - val_loss: 6.3921e-04\n",
      "Epoch 21/50\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 1.8938e-04 - val_loss: 9.4591e-04\n",
      "Epoch 22/50\n",
      "126/126 [==============================] - 1s 7ms/step - loss: 1.9900e-04 - val_loss: 6.1392e-04\n",
      "Epoch 23/50\n",
      "126/126 [==============================] - 1s 6ms/step - loss: 1.8345e-04 - val_loss: 7.4345e-04\n",
      "Epoch 24/50\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 1.8376e-04 - val_loss: 5.8430e-04\n",
      "Epoch 25/50\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 1.8405e-04 - val_loss: 5.4500e-04\n",
      "Epoch 26/50\n",
      "126/126 [==============================] - 1s 7ms/step - loss: 2.0034e-04 - val_loss: 6.0931e-04\n",
      "Epoch 27/50\n",
      "126/126 [==============================] - 2s 14ms/step - loss: 1.7146e-04 - val_loss: 5.4148e-04\n",
      "Epoch 28/50\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 1.6878e-04 - val_loss: 6.1199e-04\n",
      "Epoch 29/50\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 1.7536e-04 - val_loss: 6.6786e-04\n",
      "Epoch 30/50\n",
      "126/126 [==============================] - 1s 6ms/step - loss: 1.9775e-04 - val_loss: 5.3879e-04\n",
      "Epoch 31/50\n",
      "126/126 [==============================] - 1s 6ms/step - loss: 1.7261e-04 - val_loss: 7.9479e-04\n",
      "Epoch 32/50\n",
      "126/126 [==============================] - 1s 6ms/step - loss: 1.6077e-04 - val_loss: 5.1903e-04\n",
      "Epoch 33/50\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 1.7885e-04 - val_loss: 5.1727e-04\n",
      "Epoch 34/50\n",
      "126/126 [==============================] - 1s 5ms/step - loss: 1.8157e-04 - val_loss: 0.0016\n",
      "Epoch 35/50\n",
      "126/126 [==============================] - 1s 7ms/step - loss: 1.7049e-04 - val_loss: 6.2641e-04\n",
      "Epoch 36/50\n",
      "126/126 [==============================] - 1s 7ms/step - loss: 1.5393e-04 - val_loss: 5.7108e-04\n",
      "Epoch 37/50\n",
      "126/126 [==============================] - 1s 6ms/step - loss: 1.6719e-04 - val_loss: 8.1931e-04\n",
      "Epoch 38/50\n",
      "126/126 [==============================] - 1s 7ms/step - loss: 1.5894e-04 - val_loss: 5.1949e-04\n",
      "Epoch 39/50\n",
      "126/126 [==============================] - 1s 5ms/step - loss: 1.7312e-04 - val_loss: 5.0988e-04\n",
      "Epoch 40/50\n",
      "126/126 [==============================] - 1s 6ms/step - loss: 1.7275e-04 - val_loss: 5.0914e-04\n",
      "Epoch 41/50\n",
      "126/126 [==============================] - 1s 6ms/step - loss: 1.6429e-04 - val_loss: 5.6499e-04\n",
      "Epoch 42/50\n",
      "126/126 [==============================] - 1s 6ms/step - loss: 1.5762e-04 - val_loss: 4.9354e-04\n",
      "Epoch 43/50\n",
      "126/126 [==============================] - 1s 10ms/step - loss: 1.5274e-04 - val_loss: 5.0620e-04\n",
      "Epoch 44/50\n",
      "126/126 [==============================] - 1s 6ms/step - loss: 1.5319e-04 - val_loss: 5.9009e-04\n",
      "Epoch 45/50\n",
      "126/126 [==============================] - 1s 6ms/step - loss: 1.6599e-04 - val_loss: 5.7769e-04\n",
      "Epoch 46/50\n",
      "126/126 [==============================] - 1s 6ms/step - loss: 1.5191e-04 - val_loss: 5.3632e-04\n",
      "Epoch 47/50\n",
      "126/126 [==============================] - 1s 5ms/step - loss: 1.7308e-04 - val_loss: 5.0287e-04\n",
      "Epoch 48/50\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 1.4812e-04 - val_loss: 4.7850e-04\n",
      "Epoch 49/50\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 1.5012e-04 - val_loss: 7.2952e-04\n",
      "Epoch 50/50\n",
      "126/126 [==============================] - 2s 14ms/step - loss: 1.6369e-04 - val_loss: 4.8281e-04\n",
      "32/32 [==============================] - 1s 9ms/step\n",
      "LSTM - MSE: 5.49105497255491, MAE: 1.7191451271074727, RMSE: 2.343300017615096\n"
     ]
    }
   ],
   "source": [
    "#LSTM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('IBM.csv')\n",
    "\n",
    "# Preprocessing\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data = data.sort_values('Date')\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "data_normalized = pd.DataFrame(scaler.fit_transform(data), columns=data.columns, index=data.index)\n",
    "\n",
    "# Define a new scaler for the 'Adj Close' column\n",
    "scaler_adj_close = MinMaxScaler()\n",
    "data_normalized['Adj Close'] = scaler_adj_close.fit_transform(data[['Adj Close']])\n",
    "\n",
    "# Create the sequences\n",
    "lookback = 5\n",
    "X, Y = [], []\n",
    "for i in range(lookback, len(data_normalized)):\n",
    "    X.append(data_normalized.iloc[i-lookback:i].values)\n",
    "    Y.append(data_normalized.iloc[i]['Adj Close'])\n",
    "X, Y = np.array(X), np.array(Y)\n",
    "\n",
    "# Split data into training and test sets\n",
    "train_size = int(len(X) * 0.8)\n",
    "trainX, testX = X[:train_size], X[train_size:]\n",
    "trainY, testY = Y[:train_size], Y[train_size:]\n",
    "\n",
    "# LSTM Model\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(LSTM(50, activation='relu', input_shape=(lookback, data_normalized.shape[1])))\n",
    "model_lstm.add(Dense(1))\n",
    "model_lstm.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "history_lstm = model_lstm.fit(trainX, trainY, epochs=50, validation_data=(testX, testY), verbose=1)\n",
    "\n",
    "# Predict on the test data\n",
    "preds_lstm = model_lstm.predict(testX)\n",
    "\n",
    "# Un-normalize the predicted values\n",
    "preds_unscaled_lstm = scaler_adj_close.inverse_transform(preds_lstm)\n",
    "\n",
    "# Un-normalize the actual values\n",
    "actual_unscaled = scaler_adj_close.inverse_transform(testY.reshape(-1, 1))\n",
    "\n",
    "# Calculate and print MSE and MAE\n",
    "mse_lstm = mean_squared_error(actual_unscaled, preds_unscaled_lstm)\n",
    "mae_lstm = mean_absolute_error(actual_unscaled, preds_unscaled_lstm)\n",
    "rmse_lstm = sqrt(mean_squared_error(actual_unscaled, preds_unscaled_lstm))\n",
    "print(f'LSTM - MSE: {mse_lstm}, MAE: {mae_lstm}, RMSE: {rmse_lstm}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "13bc545e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "# Plot real vs predicted values\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(actual_unscaled, color='blue', label='Actual values')\n",
    "plt.plot(preds_unscaled_lstm, color='red', label='Predicted values')\n",
    "plt.title('IBM Stock Price Prediction')\n",
    "plt.xlabel('Time steps')\n",
    "plt.ylabel('IBM Stock Price')\n",
    "plt.legend()\n",
    "\n",
    "# Save the figure before showing\n",
    "plt.savefig('shit.png', dpi=1000)  # You can adjust DPI (dots per inch) for higher quality\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "44533705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "126/126 [==============================] - 4s 20ms/step - loss: 0.0096 - val_loss: 0.0041\n",
      "Epoch 2/50\n",
      "126/126 [==============================] - 2s 15ms/step - loss: 2.5046e-04 - val_loss: 0.0053\n",
      "Epoch 3/50\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 1.5887e-04 - val_loss: 0.0026\n",
      "Epoch 4/50\n",
      "126/126 [==============================] - 1s 5ms/step - loss: 1.1881e-04 - val_loss: 0.0017\n",
      "Epoch 5/50\n",
      "126/126 [==============================] - 1s 5ms/step - loss: 1.0601e-04 - val_loss: 0.0015\n",
      "Epoch 6/50\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 9.3846e-05 - val_loss: 0.0014\n",
      "Epoch 7/50\n",
      "126/126 [==============================] - 1s 5ms/step - loss: 8.5129e-05 - val_loss: 6.4319e-04\n",
      "Epoch 8/50\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 8.4724e-05 - val_loss: 6.1662e-04\n",
      "Epoch 9/50\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 8.3575e-05 - val_loss: 9.6061e-04\n",
      "Epoch 10/50\n",
      "126/126 [==============================] - 1s 10ms/step - loss: 7.6945e-05 - val_loss: 6.4619e-04\n",
      "Epoch 11/50\n",
      "126/126 [==============================] - 1s 10ms/step - loss: 9.3983e-05 - val_loss: 0.0018\n",
      "Epoch 12/50\n",
      "126/126 [==============================] - 3s 23ms/step - loss: 7.8840e-05 - val_loss: 7.1567e-04\n",
      "Epoch 13/50\n",
      "126/126 [==============================] - 2s 14ms/step - loss: 7.2410e-05 - val_loss: 6.3647e-04\n",
      "Epoch 14/50\n",
      "126/126 [==============================] - 2s 15ms/step - loss: 8.0944e-05 - val_loss: 0.0011\n",
      "Epoch 15/50\n",
      "126/126 [==============================] - 1s 7ms/step - loss: 7.9785e-05 - val_loss: 5.0895e-04\n",
      "Epoch 16/50\n",
      "126/126 [==============================] - 1s 7ms/step - loss: 7.0785e-05 - val_loss: 0.0014\n",
      "Epoch 17/50\n",
      "126/126 [==============================] - 1s 7ms/step - loss: 7.9904e-05 - val_loss: 5.5724e-04\n",
      "Epoch 18/50\n",
      "126/126 [==============================] - 2s 15ms/step - loss: 7.8154e-05 - val_loss: 9.1060e-04\n",
      "Epoch 19/50\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 7.3601e-05 - val_loss: 8.9944e-04\n",
      "Epoch 20/50\n",
      "126/126 [==============================] - 1s 10ms/step - loss: 7.4073e-05 - val_loss: 8.2231e-04\n",
      "Epoch 21/50\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 6.8977e-05 - val_loss: 0.0012\n",
      "Epoch 22/50\n",
      "126/126 [==============================] - 1s 7ms/step - loss: 6.4213e-05 - val_loss: 7.3335e-04\n",
      "Epoch 23/50\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 5.9838e-05 - val_loss: 6.1509e-04\n",
      "Epoch 24/50\n",
      "126/126 [==============================] - 1s 10ms/step - loss: 6.4606e-05 - val_loss: 8.9347e-04\n",
      "Epoch 25/50\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 6.0678e-05 - val_loss: 6.9430e-04\n",
      "Epoch 26/50\n",
      "126/126 [==============================] - 1s 7ms/step - loss: 6.0982e-05 - val_loss: 5.3758e-04\n",
      "Epoch 27/50\n",
      "126/126 [==============================] - 1s 10ms/step - loss: 6.0762e-05 - val_loss: 0.0011\n",
      "Epoch 28/50\n",
      "126/126 [==============================] - 1s 7ms/step - loss: 6.1186e-05 - val_loss: 0.0016\n",
      "Epoch 29/50\n",
      "126/126 [==============================] - 1s 10ms/step - loss: 6.2314e-05 - val_loss: 0.0016\n",
      "Epoch 30/50\n",
      "126/126 [==============================] - 1s 7ms/step - loss: 5.8954e-05 - val_loss: 0.0011\n",
      "Epoch 31/50\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 6.2347e-05 - val_loss: 4.1958e-04\n",
      "Epoch 32/50\n",
      "126/126 [==============================] - 1s 6ms/step - loss: 6.0320e-05 - val_loss: 4.9507e-04\n",
      "Epoch 33/50\n",
      "126/126 [==============================] - 1s 5ms/step - loss: 5.6674e-05 - val_loss: 5.9694e-04\n",
      "Epoch 34/50\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 5.5280e-05 - val_loss: 5.9171e-04\n",
      "Epoch 35/50\n",
      "126/126 [==============================] - 1s 10ms/step - loss: 6.0206e-05 - val_loss: 6.2324e-04\n",
      "Epoch 36/50\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 5.8244e-05 - val_loss: 8.4784e-04\n",
      "Epoch 37/50\n",
      "126/126 [==============================] - 1s 6ms/step - loss: 5.5454e-05 - val_loss: 5.9928e-04\n",
      "Epoch 38/50\n",
      "126/126 [==============================] - 1s 4ms/step - loss: 6.0612e-05 - val_loss: 0.0013\n",
      "Epoch 39/50\n",
      "126/126 [==============================] - 1s 7ms/step - loss: 6.1529e-05 - val_loss: 8.1291e-04\n",
      "Epoch 40/50\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 5.5354e-05 - val_loss: 0.0011\n",
      "Epoch 41/50\n",
      "126/126 [==============================] - 1s 6ms/step - loss: 5.7241e-05 - val_loss: 5.4726e-04\n",
      "Epoch 42/50\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 5.1422e-05 - val_loss: 7.5350e-04\n",
      "Epoch 43/50\n",
      "126/126 [==============================] - 1s 6ms/step - loss: 5.3773e-05 - val_loss: 4.8744e-04\n",
      "Epoch 44/50\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 6.1058e-05 - val_loss: 0.0012\n",
      "Epoch 45/50\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 5.6389e-05 - val_loss: 5.7460e-04\n",
      "Epoch 46/50\n",
      "126/126 [==============================] - 1s 10ms/step - loss: 5.2649e-05 - val_loss: 7.8467e-04\n",
      "Epoch 47/50\n",
      "126/126 [==============================] - 1s 5ms/step - loss: 6.2649e-05 - val_loss: 8.0704e-04\n",
      "Epoch 48/50\n",
      "126/126 [==============================] - 1s 10ms/step - loss: 5.7851e-05 - val_loss: 0.0015\n",
      "Epoch 49/50\n",
      "126/126 [==============================] - 2s 14ms/step - loss: 5.6837e-05 - val_loss: 0.0015\n",
      "Epoch 50/50\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 5.6805e-05 - val_loss: 7.1060e-04\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "LSTM - MSE: 1.8592711140400702, MAE: 1.1038688350231862, RMSE: 1.363550920956042\n"
     ]
    }
   ],
   "source": [
    "#LSTM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('CSCO.csv')\n",
    "\n",
    "# Preprocessing\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data = data.sort_values('Date')\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "data_normalized = pd.DataFrame(scaler.fit_transform(data), columns=data.columns, index=data.index)\n",
    "\n",
    "# Define a new scaler for the 'Adj Close' column\n",
    "scaler_adj_close = MinMaxScaler()\n",
    "data_normalized['Adj Close'] = scaler_adj_close.fit_transform(data[['Adj Close']])\n",
    "\n",
    "# Create the sequences\n",
    "lookback = 5\n",
    "X, Y = [], []\n",
    "for i in range(lookback, len(data_normalized)):\n",
    "    X.append(data_normalized.iloc[i-lookback:i].values)\n",
    "    Y.append(data_normalized.iloc[i]['Adj Close'])\n",
    "X, Y = np.array(X), np.array(Y)\n",
    "\n",
    "# Split data into training and test sets\n",
    "train_size = int(len(X) * 0.8)\n",
    "trainX, testX = X[:train_size], X[train_size:]\n",
    "trainY, testY = Y[:train_size], Y[train_size:]\n",
    "\n",
    "# LSTM Model\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(LSTM(50, activation='relu', input_shape=(lookback, data_normalized.shape[1])))\n",
    "model_lstm.add(Dense(1))\n",
    "model_lstm.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "history_lstm = model_lstm.fit(trainX, trainY, epochs=50, validation_data=(testX, testY), verbose=1)\n",
    "\n",
    "# Predict on the test data\n",
    "preds_lstm = model_lstm.predict(testX)\n",
    "\n",
    "# Un-normalize the predicted values\n",
    "preds_unscaled_lstm = scaler_adj_close.inverse_transform(preds_lstm)\n",
    "\n",
    "# Un-normalize the actual values\n",
    "actual_unscaled = scaler_adj_close.inverse_transform(testY.reshape(-1, 1))\n",
    "\n",
    "# Calculate and print MSE and MAE\n",
    "mse_lstm = mean_squared_error(actual_unscaled, preds_unscaled_lstm)\n",
    "mae_lstm = mean_absolute_error(actual_unscaled, preds_unscaled_lstm)\n",
    "rmse_lstm = sqrt(mean_squared_error(actual_unscaled, preds_unscaled_lstm))\n",
    "print(f'LSTM - MSE: {mse_lstm}, MAE: {mae_lstm}, RMSE: {rmse_lstm}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "31a387c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "# Plot real vs predicted values\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(actual_unscaled, color='blue', label='Actual values')\n",
    "plt.plot(preds_unscaled_lstm, color='red', label='Predicted values')\n",
    "plt.title('CSCO Stock Price Prediction')\n",
    "plt.xlabel('Time steps')\n",
    "plt.ylabel('CSCO Stock Price')\n",
    "plt.legend()\n",
    "\n",
    "# Save the figure before showing\n",
    "plt.savefig('shit2.png', dpi=1000)  # You can adjust DPI (dots per inch) for higher quality\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d7365c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "126/126 [==============================] - 3s 8ms/step - loss: 0.0062 - val_loss: 0.0056\n",
      "Epoch 2/50\n",
      "126/126 [==============================] - 1s 7ms/step - loss: 5.3645e-04 - val_loss: 0.0044\n",
      "Epoch 3/50\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 3.5173e-04 - val_loss: 0.0025\n",
      "Epoch 4/50\n",
      "126/126 [==============================] - 1s 7ms/step - loss: 1.8787e-04 - val_loss: 0.0014\n",
      "Epoch 5/50\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 1.2019e-04 - val_loss: 9.6721e-04\n",
      "Epoch 6/50\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 1.1477e-04 - val_loss: 8.9950e-04\n",
      "Epoch 7/50\n",
      "126/126 [==============================] - 1s 6ms/step - loss: 1.0366e-04 - val_loss: 7.6115e-04\n",
      "Epoch 8/50\n",
      "126/126 [==============================] - 1s 7ms/step - loss: 9.4941e-05 - val_loss: 6.8396e-04\n",
      "Epoch 9/50\n",
      "126/126 [==============================] - 1s 6ms/step - loss: 9.9277e-05 - val_loss: 6.9625e-04\n",
      "Epoch 10/50\n",
      "126/126 [==============================] - 1s 7ms/step - loss: 8.8803e-05 - val_loss: 6.0680e-04\n",
      "Epoch 11/50\n",
      "126/126 [==============================] - 1s 7ms/step - loss: 9.2313e-05 - val_loss: 6.1535e-04\n",
      "Epoch 12/50\n",
      "126/126 [==============================] - 1s 6ms/step - loss: 8.9988e-05 - val_loss: 7.5970e-04\n",
      "Epoch 13/50\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 8.9405e-05 - val_loss: 6.0740e-04\n",
      "Epoch 14/50\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 8.9647e-05 - val_loss: 6.4011e-04\n",
      "Epoch 15/50\n",
      "126/126 [==============================] - 1s 7ms/step - loss: 9.0665e-05 - val_loss: 5.6892e-04\n",
      "Epoch 16/50\n",
      "126/126 [==============================] - 1s 7ms/step - loss: 8.9987e-05 - val_loss: 0.0010\n",
      "Epoch 17/50\n",
      "126/126 [==============================] - 1s 6ms/step - loss: 8.8456e-05 - val_loss: 5.3962e-04\n",
      "Epoch 18/50\n",
      "126/126 [==============================] - 1s 6ms/step - loss: 9.4878e-05 - val_loss: 5.5252e-04\n",
      "Epoch 19/50\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 7.8815e-05 - val_loss: 6.2096e-04\n",
      "Epoch 20/50\n",
      "126/126 [==============================] - 1s 6ms/step - loss: 8.0845e-05 - val_loss: 9.5453e-04\n",
      "Epoch 21/50\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 8.8451e-05 - val_loss: 5.6316e-04\n",
      "Epoch 22/50\n",
      "126/126 [==============================] - 1s 7ms/step - loss: 7.5047e-05 - val_loss: 5.2711e-04\n",
      "Epoch 23/50\n",
      "126/126 [==============================] - 1s 7ms/step - loss: 9.0364e-05 - val_loss: 6.3031e-04\n",
      "Epoch 24/50\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 7.5835e-05 - val_loss: 4.9595e-04\n",
      "Epoch 25/50\n",
      "126/126 [==============================] - 1s 7ms/step - loss: 7.6342e-05 - val_loss: 6.2105e-04\n",
      "Epoch 26/50\n",
      "126/126 [==============================] - 1s 6ms/step - loss: 7.3272e-05 - val_loss: 5.1023e-04\n",
      "Epoch 27/50\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 7.2451e-05 - val_loss: 5.7264e-04\n",
      "Epoch 28/50\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 8.6776e-05 - val_loss: 5.1577e-04\n",
      "Epoch 29/50\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 8.9415e-05 - val_loss: 4.9980e-04\n",
      "Epoch 30/50\n",
      "126/126 [==============================] - 1s 7ms/step - loss: 7.7553e-05 - val_loss: 6.1089e-04\n",
      "Epoch 31/50\n",
      "126/126 [==============================] - 1s 7ms/step - loss: 7.3847e-05 - val_loss: 5.0688e-04\n",
      "Epoch 32/50\n",
      "126/126 [==============================] - 1s 7ms/step - loss: 7.2290e-05 - val_loss: 5.0265e-04\n",
      "Epoch 33/50\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 7.1683e-05 - val_loss: 5.6301e-04\n",
      "Epoch 34/50\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 7.2489e-05 - val_loss: 4.9310e-04\n",
      "Epoch 35/50\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 6.7455e-05 - val_loss: 4.9496e-04\n",
      "Epoch 36/50\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 9.2693e-05 - val_loss: 5.1786e-04\n",
      "Epoch 37/50\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 7.4815e-05 - val_loss: 5.6282e-04\n",
      "Epoch 38/50\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 7.9562e-05 - val_loss: 4.9448e-04\n",
      "Epoch 39/50\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 6.9985e-05 - val_loss: 4.9193e-04\n",
      "Epoch 40/50\n",
      "126/126 [==============================] - 1s 10ms/step - loss: 6.8285e-05 - val_loss: 5.4372e-04\n",
      "Epoch 41/50\n",
      "126/126 [==============================] - 1s 10ms/step - loss: 6.7876e-05 - val_loss: 6.3236e-04\n",
      "Epoch 42/50\n",
      "126/126 [==============================] - 1s 10ms/step - loss: 7.8116e-05 - val_loss: 5.0910e-04\n",
      "Epoch 43/50\n",
      "126/126 [==============================] - 1s 11ms/step - loss: 6.8146e-05 - val_loss: 5.0066e-04\n",
      "Epoch 44/50\n",
      "126/126 [==============================] - 1s 10ms/step - loss: 6.9891e-05 - val_loss: 5.7003e-04\n",
      "Epoch 45/50\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 6.8332e-05 - val_loss: 5.2342e-04\n",
      "Epoch 46/50\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 8.2644e-05 - val_loss: 6.4298e-04\n",
      "Epoch 47/50\n",
      "126/126 [==============================] - 1s 10ms/step - loss: 7.3985e-05 - val_loss: 4.6469e-04\n",
      "Epoch 48/50\n",
      "126/126 [==============================] - 2s 12ms/step - loss: 6.0712e-05 - val_loss: 4.6724e-04\n",
      "Epoch 49/50\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 6.4830e-05 - val_loss: 5.2514e-04\n",
      "Epoch 50/50\n",
      "126/126 [==============================] - 1s 10ms/step - loss: 6.7681e-05 - val_loss: 4.6947e-04\n",
      "32/32 [==============================] - 1s 7ms/step\n",
      "LSTM - MSE: 1.448764767428652, MAE: 0.8536615614918887, RMSE: 1.2036464461911778\n"
     ]
    }
   ],
   "source": [
    "#LSTM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('INTC.csv')\n",
    "\n",
    "# Preprocessing\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data = data.sort_values('Date')\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "data_normalized = pd.DataFrame(scaler.fit_transform(data), columns=data.columns, index=data.index)\n",
    "\n",
    "# Define a new scaler for the 'Adj Close' column\n",
    "scaler_adj_close = MinMaxScaler()\n",
    "data_normalized['Adj Close'] = scaler_adj_close.fit_transform(data[['Adj Close']])\n",
    "\n",
    "# Create the sequences\n",
    "lookback = 5\n",
    "X, Y = [], []\n",
    "for i in range(lookback, len(data_normalized)):\n",
    "    X.append(data_normalized.iloc[i-lookback:i].values)\n",
    "    Y.append(data_normalized.iloc[i]['Adj Close'])\n",
    "X, Y = np.array(X), np.array(Y)\n",
    "\n",
    "# Split data into training and test sets\n",
    "train_size = int(len(X) * 0.8)\n",
    "trainX, testX = X[:train_size], X[train_size:]\n",
    "trainY, testY = Y[:train_size], Y[train_size:]\n",
    "\n",
    "# LSTM Model\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(LSTM(50, activation='relu', input_shape=(lookback, data_normalized.shape[1])))\n",
    "model_lstm.add(Dense(1))\n",
    "model_lstm.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "history_lstm = model_lstm.fit(trainX, trainY, epochs=50, validation_data=(testX, testY), verbose=1)\n",
    "\n",
    "# Predict on the test data\n",
    "preds_lstm = model_lstm.predict(testX)\n",
    "\n",
    "# Un-normalize the predicted values\n",
    "preds_unscaled_lstm = scaler_adj_close.inverse_transform(preds_lstm)\n",
    "\n",
    "# Un-normalize the actual values\n",
    "actual_unscaled = scaler_adj_close.inverse_transform(testY.reshape(-1, 1))\n",
    "\n",
    "# Calculate and print MSE and MAE\n",
    "mse_lstm = mean_squared_error(actual_unscaled, preds_unscaled_lstm)\n",
    "mae_lstm = mean_absolute_error(actual_unscaled, preds_unscaled_lstm)\n",
    "rmse_lstm = sqrt(mean_squared_error(actual_unscaled, preds_unscaled_lstm))\n",
    "print(f'LSTM - MSE: {mse_lstm}, MAE: {mae_lstm}, RMSE: {rmse_lstm}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f97356eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0b/xnr8n6zn4ps83bsx5jmhm1yh0000gn/T/ipykernel_39907/2503375032.py:17: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "# Plot real vs predicted values\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(actual_unscaled, color='blue', label='Actual values')\n",
    "plt.plot(preds_unscaled_lstm, color='red', label='Predicted values')\n",
    "plt.title('INTC Stock Price Prediction')\n",
    "plt.xlabel('Time steps')\n",
    "plt.ylabel('INTC Stock Price')\n",
    "plt.legend()\n",
    "\n",
    "# Save the figure before showing\n",
    "plt.savefig('shit3.png', dpi=1000)  # You can adjust DPI (dots per inch) for higher quality\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d2594e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Simple Exponential Smoothing</td>\n",
       "      <td>10.935615</td>\n",
       "      <td>12.988858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Triple Exponential Smoothing (Holt-Winters)</td>\n",
       "      <td>11.591645</td>\n",
       "      <td>14.235196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Double Exponential Smoothing (Holt)</td>\n",
       "      <td>11.782964</td>\n",
       "      <td>14.416923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>23.917686</td>\n",
       "      <td>25.156948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>24.061488</td>\n",
       "      <td>25.613175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>24.365869</td>\n",
       "      <td>26.050204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Extra Trees</td>\n",
       "      <td>24.439074</td>\n",
       "      <td>26.086205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bagging</td>\n",
       "      <td>24.391316</td>\n",
       "      <td>26.114960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>KNN (k=10)</td>\n",
       "      <td>24.375641</td>\n",
       "      <td>26.200670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MLP</td>\n",
       "      <td>25.165119</td>\n",
       "      <td>26.262216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>KNN (k=5)</td>\n",
       "      <td>24.437397</td>\n",
       "      <td>26.330655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>KNN (k=3)</td>\n",
       "      <td>24.360512</td>\n",
       "      <td>26.349772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>24.391583</td>\n",
       "      <td>26.964484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>SVR (RBF)</td>\n",
       "      <td>27.177603</td>\n",
       "      <td>28.255468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>30.579160</td>\n",
       "      <td>31.531077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Ridge Regression</td>\n",
       "      <td>30.579498</td>\n",
       "      <td>31.531367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Elastic Net</td>\n",
       "      <td>30.953202</td>\n",
       "      <td>31.866271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Lasso Regression</td>\n",
       "      <td>30.957550</td>\n",
       "      <td>31.873394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>SVR (Poly)</td>\n",
       "      <td>32.406608</td>\n",
       "      <td>34.617771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>SVR (Linear)</td>\n",
       "      <td>34.169380</td>\n",
       "      <td>34.990239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Model        MAE       RMSE\n",
       "0                  Simple Exponential Smoothing  10.935615  12.988858\n",
       "1   Triple Exponential Smoothing (Holt-Winters)  11.591645  14.235196\n",
       "2           Double Exponential Smoothing (Holt)  11.782964  14.416923\n",
       "3                                      AdaBoost  23.917686  25.156948\n",
       "4                             Gradient Boosting  24.061488  25.613175\n",
       "5                                 Random Forest  24.365869  26.050204\n",
       "6                                   Extra Trees  24.439074  26.086205\n",
       "7                                       Bagging  24.391316  26.114960\n",
       "8                                    KNN (k=10)  24.375641  26.200670\n",
       "9                                           MLP  25.165119  26.262216\n",
       "10                                    KNN (k=5)  24.437397  26.330655\n",
       "11                                    KNN (k=3)  24.360512  26.349772\n",
       "12                                Decision Tree  24.391583  26.964484\n",
       "13                                    SVR (RBF)  27.177603  28.255468\n",
       "14                            Linear Regression  30.579160  31.531077\n",
       "15                             Ridge Regression  30.579498  31.531367\n",
       "16                                  Elastic Net  30.953202  31.866271\n",
       "17                             Lasso Regression  30.957550  31.873394\n",
       "18                                   SVR (Poly)  32.406608  34.617771\n",
       "19                                 SVR (Linear)  34.169380  34.990239"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from statsmodels.tsa.api import SimpleExpSmoothing, Holt, ExponentialSmoothing\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Prepare the data\n",
    "data = pd.read_csv('IBM.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "X = data.drop('Adj Close', axis=1)\n",
    "y = data['Adj Close']\n",
    "split_index = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split_index], X[split_index:]\n",
    "y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "# Scale the data for models that require it\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Set up the models\n",
    "models = [\n",
    "    (\"Linear Regression\", LinearRegression(), X_train, y_train, X_test, y_test),\n",
    "    (\"Ridge Regression\", Ridge(), X_train, y_train, X_test, y_test),\n",
    "    (\"Lasso Regression\", Lasso(), X_train, y_train, X_test, y_test),\n",
    "    (\"Elastic Net\", ElasticNet(), X_train, y_train, X_test, y_test),\n",
    "    (\"SVR (Linear)\", SVR(kernel='linear'), X_train_scaled, y_train, X_test_scaled, y_test),\n",
    "    (\"SVR (Poly)\", SVR(kernel='poly'), X_train_scaled, y_train, X_test_scaled, y_test),\n",
    "    (\"SVR (RBF)\", SVR(kernel='rbf'), X_train_scaled, y_train, X_test_scaled, y_test),\n",
    "    (\"KNN (k=3)\", KNeighborsRegressor(n_neighbors=3), X_train_scaled, y_train, X_test_scaled, y_test),\n",
    "    (\"KNN (k=5)\", KNeighborsRegressor(n_neighbors=5), X_train_scaled, y_train, X_test_scaled, y_test),\n",
    "    (\"KNN (k=10)\", KNeighborsRegressor(n_neighbors=10), X_train_scaled, y_train, X_test_scaled, y_test),\n",
    "    (\"Decision Tree\", DecisionTreeRegressor(), X_train, y_train, X_test, y_test),\n",
    "    (\"Random Forest\", RandomForestRegressor(), X_train, y_train, X_test, y_test),\n",
    "    (\"Extra Trees\", ExtraTreesRegressor(), X_train, y_train, X_test, y_test),\n",
    "    (\"Gradient Boosting\", GradientBoostingRegressor(), X_train, y_train, X_test, y_test),\n",
    "    (\"AdaBoost\", AdaBoostRegressor(), X_train, y_train, X_test, y_test),\n",
    "    (\"Bagging\", BaggingRegressor(), X_train, y_train, X_test, y_test),\n",
    "    (\"MLP\", MLPRegressor(max_iter=500), X_train_scaled, y_train, X_test_scaled, y_test),\n",
    "    (\"Simple Exponential Smoothing\", SimpleExpSmoothing(y_train).fit(), None, None, y_test, y_test),\n",
    "    (\"Double Exponential Smoothing (Holt)\", Holt(y_train).fit(), None, None, y_test, y_test),\n",
    "    (\"Triple Exponential Smoothing (Holt-Winters)\", ExponentialSmoothing(y_train, seasonal_periods=12, trend='add', seasonal='add').fit(), None, None, y_test, y_test),\n",
    "]\n",
    "\n",
    "# Run each model and collect the results\n",
    "results = []\n",
    "for name, model, X_train, y_train, X_test, y_test in models:\n",
    "    if name not in [\"Simple Exponential Smoothing\", \"Double Exponential Smoothing (Holt)\", \n",
    "                    \"Triple Exponential Smoothing (Holt-Winters)\", \"AR\", \"MA\", \"ARMA\", \"ARIMA\", \"SARIMA\"]:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "    else:\n",
    "        y_pred = model.forecast(steps=len(y_test))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    results.append([name, mae, rmse])\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "results_df = pd.DataFrame(results, columns=['Model', 'MAE', 'RMSE'])\n",
    "\n",
    "# Sort the results by RMSE\n",
    "results_df.sort_values('RMSE', inplace=True)\n",
    "results_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save the results to an Excel file\n",
    "results_df.to_excel(\"model_results.xlsx\", index=False)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0ecbcbb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP</td>\n",
       "      <td>3.692254</td>\n",
       "      <td>4.001763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>3.638300</td>\n",
       "      <td>4.009251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Extra Trees</td>\n",
       "      <td>3.656191</td>\n",
       "      <td>4.022862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bagging</td>\n",
       "      <td>3.663573</td>\n",
       "      <td>4.026436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>3.663127</td>\n",
       "      <td>4.027455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>3.684761</td>\n",
       "      <td>4.049399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>KNN (k=3)</td>\n",
       "      <td>3.700417</td>\n",
       "      <td>4.081816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SVR (Linear)</td>\n",
       "      <td>3.786542</td>\n",
       "      <td>4.091680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>KNN (k=5)</td>\n",
       "      <td>3.708482</td>\n",
       "      <td>4.091917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>KNN (k=10)</td>\n",
       "      <td>3.711898</td>\n",
       "      <td>4.104029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>3.714869</td>\n",
       "      <td>4.129608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Ridge Regression</td>\n",
       "      <td>4.254293</td>\n",
       "      <td>4.528333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>4.254349</td>\n",
       "      <td>4.528362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Elastic Net</td>\n",
       "      <td>4.431428</td>\n",
       "      <td>4.710163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SVR (RBF)</td>\n",
       "      <td>4.105688</td>\n",
       "      <td>4.784849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Lasso Regression</td>\n",
       "      <td>4.528794</td>\n",
       "      <td>4.802968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Simple Exponential Smoothing</td>\n",
       "      <td>5.989388</td>\n",
       "      <td>7.294086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Triple Exponential Smoothing (Holt-Winters)</td>\n",
       "      <td>9.153540</td>\n",
       "      <td>10.290326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Double Exponential Smoothing (Holt)</td>\n",
       "      <td>9.196367</td>\n",
       "      <td>10.330918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>SVR (Poly)</td>\n",
       "      <td>9.535232</td>\n",
       "      <td>12.083935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Model       MAE       RMSE\n",
       "0                                           MLP  3.692254   4.001763\n",
       "1                                 Decision Tree  3.638300   4.009251\n",
       "2                                   Extra Trees  3.656191   4.022862\n",
       "3                                       Bagging  3.663573   4.026436\n",
       "4                                 Random Forest  3.663127   4.027455\n",
       "5                             Gradient Boosting  3.684761   4.049399\n",
       "6                                     KNN (k=3)  3.700417   4.081816\n",
       "7                                  SVR (Linear)  3.786542   4.091680\n",
       "8                                     KNN (k=5)  3.708482   4.091917\n",
       "9                                    KNN (k=10)  3.711898   4.104029\n",
       "10                                     AdaBoost  3.714869   4.129608\n",
       "11                             Ridge Regression  4.254293   4.528333\n",
       "12                            Linear Regression  4.254349   4.528362\n",
       "13                                  Elastic Net  4.431428   4.710163\n",
       "14                                    SVR (RBF)  4.105688   4.784849\n",
       "15                             Lasso Regression  4.528794   4.802968\n",
       "16                 Simple Exponential Smoothing  5.989388   7.294086\n",
       "17  Triple Exponential Smoothing (Holt-Winters)  9.153540  10.290326\n",
       "18          Double Exponential Smoothing (Holt)  9.196367  10.330918\n",
       "19                                   SVR (Poly)  9.535232  12.083935"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from statsmodels.tsa.api import SimpleExpSmoothing, Holt, ExponentialSmoothing\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Prepare the data\n",
    "data = pd.read_csv('CSCO.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "X = data.drop('Adj Close', axis=1)\n",
    "y = data['Adj Close']\n",
    "split_index = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split_index], X[split_index:]\n",
    "y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "# Scale the data for models that require it\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Set up the models\n",
    "models = [\n",
    "    (\"Linear Regression\", LinearRegression(), X_train, y_train, X_test, y_test),\n",
    "    (\"Ridge Regression\", Ridge(), X_train, y_train, X_test, y_test),\n",
    "    (\"Lasso Regression\", Lasso(), X_train, y_train, X_test, y_test),\n",
    "    (\"Elastic Net\", ElasticNet(), X_train, y_train, X_test, y_test),\n",
    "    (\"SVR (Linear)\", SVR(kernel='linear'), X_train_scaled, y_train, X_test_scaled, y_test),\n",
    "    (\"SVR (Poly)\", SVR(kernel='poly'), X_train_scaled, y_train, X_test_scaled, y_test),\n",
    "    (\"SVR (RBF)\", SVR(kernel='rbf'), X_train_scaled, y_train, X_test_scaled, y_test),\n",
    "    (\"KNN (k=3)\", KNeighborsRegressor(n_neighbors=3), X_train_scaled, y_train, X_test_scaled, y_test),\n",
    "    (\"KNN (k=5)\", KNeighborsRegressor(n_neighbors=5), X_train_scaled, y_train, X_test_scaled, y_test),\n",
    "    (\"KNN (k=10)\", KNeighborsRegressor(n_neighbors=10), X_train_scaled, y_train, X_test_scaled, y_test),\n",
    "    (\"Decision Tree\", DecisionTreeRegressor(), X_train, y_train, X_test, y_test),\n",
    "    (\"Random Forest\", RandomForestRegressor(), X_train, y_train, X_test, y_test),\n",
    "    (\"Extra Trees\", ExtraTreesRegressor(), X_train, y_train, X_test, y_test),\n",
    "    (\"Gradient Boosting\", GradientBoostingRegressor(), X_train, y_train, X_test, y_test),\n",
    "    (\"AdaBoost\", AdaBoostRegressor(), X_train, y_train, X_test, y_test),\n",
    "    (\"Bagging\", BaggingRegressor(), X_train, y_train, X_test, y_test),\n",
    "    (\"MLP\", MLPRegressor(max_iter=500), X_train_scaled, y_train, X_test_scaled, y_test),\n",
    "    (\"Simple Exponential Smoothing\", SimpleExpSmoothing(y_train).fit(), None, None, y_test, y_test),\n",
    "    (\"Double Exponential Smoothing (Holt)\", Holt(y_train).fit(), None, None, y_test, y_test),\n",
    "    (\"Triple Exponential Smoothing (Holt-Winters)\", ExponentialSmoothing(y_train, seasonal_periods=12, trend='add', seasonal='add').fit(), None, None, y_test, y_test),\n",
    "]\n",
    "\n",
    "# Run each model and collect the results\n",
    "results = []\n",
    "for name, model, X_train, y_train, X_test, y_test in models:\n",
    "    if name not in [\"Simple Exponential Smoothing\", \"Double Exponential Smoothing (Holt)\", \n",
    "                    \"Triple Exponential Smoothing (Holt-Winters)\", \"AR\", \"MA\", \"ARMA\", \"ARIMA\", \"SARIMA\"]:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "    else:\n",
    "        y_pred = model.forecast(steps=len(y_test))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    results.append([name, mae, rmse])\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "results_df = pd.DataFrame(results, columns=['Model', 'MAE', 'RMSE'])\n",
    "\n",
    "# Sort the results by RMSE\n",
    "results_df.sort_values('RMSE', inplace=True)\n",
    "results_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save the results to an Excel file\n",
    "results_df.to_excel(\"model_results.xlsx\", index=False)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "728b5963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVR (Linear)</td>\n",
       "      <td>3.478300</td>\n",
       "      <td>4.051086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>3.983808</td>\n",
       "      <td>4.458578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ridge Regression</td>\n",
       "      <td>3.984611</td>\n",
       "      <td>4.459282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lasso Regression</td>\n",
       "      <td>4.297882</td>\n",
       "      <td>4.696409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Elastic Net</td>\n",
       "      <td>4.273418</td>\n",
       "      <td>4.698660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MLP</td>\n",
       "      <td>3.912697</td>\n",
       "      <td>4.827016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Extra Trees</td>\n",
       "      <td>4.346648</td>\n",
       "      <td>5.083749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bagging</td>\n",
       "      <td>4.402657</td>\n",
       "      <td>5.140275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>4.388128</td>\n",
       "      <td>5.141680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>4.368693</td>\n",
       "      <td>5.216746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>4.522519</td>\n",
       "      <td>5.270781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>KNN (k=3)</td>\n",
       "      <td>4.515560</td>\n",
       "      <td>5.308729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>KNN (k=5)</td>\n",
       "      <td>4.581949</td>\n",
       "      <td>5.350196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>KNN (k=10)</td>\n",
       "      <td>4.779944</td>\n",
       "      <td>5.532951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>4.994082</td>\n",
       "      <td>5.770837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SVR (RBF)</td>\n",
       "      <td>6.750369</td>\n",
       "      <td>8.659797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Simple Exponential Smoothing</td>\n",
       "      <td>7.975298</td>\n",
       "      <td>9.984624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Triple Exponential Smoothing (Holt-Winters)</td>\n",
       "      <td>9.570986</td>\n",
       "      <td>12.853606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Double Exponential Smoothing (Holt)</td>\n",
       "      <td>9.582094</td>\n",
       "      <td>12.874500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>SVR (Poly)</td>\n",
       "      <td>19.649826</td>\n",
       "      <td>27.605203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Model        MAE       RMSE\n",
       "0                                  SVR (Linear)   3.478300   4.051086\n",
       "1                             Linear Regression   3.983808   4.458578\n",
       "2                              Ridge Regression   3.984611   4.459282\n",
       "3                              Lasso Regression   4.297882   4.696409\n",
       "4                                   Elastic Net   4.273418   4.698660\n",
       "5                                           MLP   3.912697   4.827016\n",
       "6                                   Extra Trees   4.346648   5.083749\n",
       "7                                       Bagging   4.402657   5.140275\n",
       "8                                 Random Forest   4.388128   5.141680\n",
       "9                                 Decision Tree   4.368693   5.216746\n",
       "10                            Gradient Boosting   4.522519   5.270781\n",
       "11                                    KNN (k=3)   4.515560   5.308729\n",
       "12                                    KNN (k=5)   4.581949   5.350196\n",
       "13                                   KNN (k=10)   4.779944   5.532951\n",
       "14                                     AdaBoost   4.994082   5.770837\n",
       "15                                    SVR (RBF)   6.750369   8.659797\n",
       "16                 Simple Exponential Smoothing   7.975298   9.984624\n",
       "17  Triple Exponential Smoothing (Holt-Winters)   9.570986  12.853606\n",
       "18          Double Exponential Smoothing (Holt)   9.582094  12.874500\n",
       "19                                   SVR (Poly)  19.649826  27.605203"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Alternatives\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from statsmodels.tsa.api import SimpleExpSmoothing, Holt, ExponentialSmoothing\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Prepare the data\n",
    "data = pd.read_csv('INTC.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "X = data.drop('Adj Close', axis=1)\n",
    "y = data['Adj Close']\n",
    "split_index = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split_index], X[split_index:]\n",
    "y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "# Scale the data for models that require it\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Set up the models\n",
    "models = [\n",
    "    (\"Linear Regression\", LinearRegression(), X_train, y_train, X_test, y_test),\n",
    "    (\"Ridge Regression\", Ridge(), X_train, y_train, X_test, y_test),\n",
    "    (\"Lasso Regression\", Lasso(), X_train, y_train, X_test, y_test),\n",
    "    (\"Elastic Net\", ElasticNet(), X_train, y_train, X_test, y_test),\n",
    "    (\"SVR (Linear)\", SVR(kernel='linear'), X_train_scaled, y_train, X_test_scaled, y_test),\n",
    "    (\"SVR (Poly)\", SVR(kernel='poly'), X_train_scaled, y_train, X_test_scaled, y_test),\n",
    "    (\"SVR (RBF)\", SVR(kernel='rbf'), X_train_scaled, y_train, X_test_scaled, y_test),\n",
    "    (\"KNN (k=3)\", KNeighborsRegressor(n_neighbors=3), X_train_scaled, y_train, X_test_scaled, y_test),\n",
    "    (\"KNN (k=5)\", KNeighborsRegressor(n_neighbors=5), X_train_scaled, y_train, X_test_scaled, y_test),\n",
    "    (\"KNN (k=10)\", KNeighborsRegressor(n_neighbors=10), X_train_scaled, y_train, X_test_scaled, y_test),\n",
    "    (\"Decision Tree\", DecisionTreeRegressor(), X_train, y_train, X_test, y_test),\n",
    "    (\"Random Forest\", RandomForestRegressor(), X_train, y_train, X_test, y_test),\n",
    "    (\"Extra Trees\", ExtraTreesRegressor(), X_train, y_train, X_test, y_test),\n",
    "    (\"Gradient Boosting\", GradientBoostingRegressor(), X_train, y_train, X_test, y_test),\n",
    "    (\"AdaBoost\", AdaBoostRegressor(), X_train, y_train, X_test, y_test),\n",
    "    (\"Bagging\", BaggingRegressor(), X_train, y_train, X_test, y_test),\n",
    "    (\"MLP\", MLPRegressor(max_iter=500), X_train_scaled, y_train, X_test_scaled, y_test),\n",
    "    (\"Simple Exponential Smoothing\", SimpleExpSmoothing(y_train).fit(), None, None, y_test, y_test),\n",
    "    (\"Double Exponential Smoothing (Holt)\", Holt(y_train).fit(), None, None, y_test, y_test),\n",
    "    (\"Triple Exponential Smoothing (Holt-Winters)\", ExponentialSmoothing(y_train, seasonal_periods=12, trend='add', seasonal='add').fit(), None, None, y_test, y_test),\n",
    "]\n",
    "\n",
    "# Run each model and collect the results\n",
    "results = []\n",
    "for name, model, X_train, y_train, X_test, y_test in models:\n",
    "    if name not in [\"Simple Exponential Smoothing\", \"Double Exponential Smoothing (Holt)\", \n",
    "                    \"Triple Exponential Smoothing (Holt-Winters)\", \"AR\", \"MA\", \"ARMA\", \"ARIMA\", \"SARIMA\"]:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "    else:\n",
    "        y_pred = model.forecast(steps=len(y_test))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    results.append([name, mae, rmse])\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "results_df = pd.DataFrame(results, columns=['Model', 'MAE', 'RMSE'])\n",
    "\n",
    "# Sort the results by RMSE\n",
    "results_df.sort_values('RMSE', inplace=True)\n",
    "results_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save the results to an Excel file\n",
    "results_df.to_excel(\"model_results.xlsx\", index=False)\n",
    "\n",
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
